{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Package instalation"
      ],
      "metadata": {
        "id": "kzsoGI9bArNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feature_engine\n",
        "!pip install tabgan"
      ],
      "metadata": {
        "id": "1a9yX5RgAJ_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install variants\n",
        "!pip install smote_variants"
      ],
      "metadata": {
        "id": "hzpskW8cAL-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "dfb363dVAxdg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmirlPO52IJC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import time\n",
        "import collections \n",
        "from matplotlib import pyplot as plt\n",
        "import argparse\n",
        "import json\n",
        "import scipy.io as sio\n",
        "from sklearn import metrics, preprocessing\n",
        "from feature_engine.encoding import CountFrequencyEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "import tensorflow as tf \n",
        "tf.config.experimental_run_functions_eagerly(True)\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.decomposition import PCA\n",
        "import variants as variants\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import smote_variants as sv\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import ks_2samp\n",
        "from scipy.spatial import distance\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from tabgan.sampler import OriginalGenerator, GANGenerator\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WGAN and related functions"
      ],
      "metadata": {
        "id": "kUALAcBVA8i9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCUZfqz829al"
      },
      "outputs": [],
      "source": [
        "# generator\n",
        "class Generator(tf.keras.Model):\n",
        "  def __init__(self,n_inp,n_noise,n_hid=128):\n",
        "    super().__init__()\n",
        "    init=tf.keras.initializers.GlorotUniform\n",
        "    self.input_layer=Dense(units=n_noise,kernel_initializer=init)\n",
        "    self.hidden_layer=Dense(units=n_hid,activation=\"relu\",kernel_initializer=init)\n",
        "    self.output_layer=Dense(units=n_inp,activation=\"sigmoid\",kernel_initializer=init)\n",
        "  def call(self,inputs):\n",
        "    x=self.input_layer(inputs)\n",
        "    x=self.hidden_layer(x)\n",
        "    return self.output_layer(x)\n",
        "# critic   \n",
        "class Critic(tf.keras.Model):\n",
        "  def __init__(self,n_inp,n_hid=128):\n",
        "    super().__init__()\n",
        "    init=tf.keras.initializers.GlorotUniform\n",
        "    self.input_layer=Dense(units=n_inp,kernel_initializer=init)\n",
        "    self.hidden_layer=Dense(units=n_hid,activation=\"relu\",kernel_initializer=init)\n",
        "    self.logits=Dense(units=1,activation=None,kernel_initializer=init)\n",
        "    \n",
        "  def call(self,inputs):\n",
        "    x=self.input_layer(inputs)\n",
        "    x=self.hidden_layer(x)\n",
        "    return self.logits(x)\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_data,gen,critic,noise_dim,generator_optimizer,critic_optimizer):\n",
        "  batch_size=real_data.shape[0]# gaussian noise :z\n",
        "  noise=tf.random.normal([batch_size,noise_dim])\n",
        "  with tf.GradientTape() as gen_tape,tf.GradientTape() as critic_tape:# x' = G(z)\n",
        "    fake_data=gen(noise,training=True)# s^ = c(x)\n",
        "    real_output=critic(real_data,training=True)# s_ = c(x')\n",
        "    fake_output=critic(fake_data,training=True)\n",
        "    critic_loss=tf.reduce_mean(fake_output)-tf.reduce_mean(real_output)\n",
        "    critic_loss_real=tf.reduce_mean(real_output)\n",
        "    critic_loss_fake=tf.reduce_mean(fake_output)# G loss fucntion is the critic's output for fake data -(s_)\n",
        "    gen_loss=-tf.reduce_mean(fake_output)\n",
        "  wasserstein=tf.reduce_mean(real_output)-tf.reduce_mean(fake_output)# calculate gradients for gen and critic to update them weights\n",
        "  gradients_of_generator=gen_tape.gradient(gen_loss,gen.trainable_variables)\n",
        "  gradients_of_critic=critic_tape.gradient(critic_loss,critic.trainable_variables)# update gen and critic weights \n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator,gen.trainable_variables))\n",
        "  critic_optimizer.apply_gradients(zip(gradients_of_critic,critic.trainable_variables))\n",
        "  tf.group(*(var.assign(tf.clip_by_value(var,-0.01,0.01)) for var in critic.trainable_variables)) \n",
        "  return wasserstein,gen_loss,critic_loss_real,critic_loss_fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiSClxsa42NV"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_samples(generator,class_id,headers_name,nb_instance,NOISE_DIM):\n",
        "  # generete instances\n",
        "  fake_data=generator(tf.random.normal([nb_instance,NOISE_DIM]))\n",
        "  # prepare syhtentic dataset for export\n",
        "  synthetic_data=pd.DataFrame(data=np.array(fake_data),columns=headers_name)\n",
        "  synthetic_data[\"0\"]=np.repeat(class_id,len(fake_data))\n",
        "  # synthetic_data.to_csv(\"GAN_Synthetic_Data\"+str(class_id)+\".csv\",index=False,header=True)\n",
        "  return synthetic_data\n",
        "def fake_data_generation(training_data,nb_instances_to_generate,target):\n",
        "  # setting training parameters for GAN\n",
        "  BATCH_SIZE=8\n",
        "  NOISE_DIM=10\n",
        "  learning_rate=0.001\n",
        "  epochs=150# save column names for later\n",
        "  headers_name=list(training_data.columns.values)\n",
        "  headers_name=headers_name[0:-1]# prepre training data\n",
        "  # class_id=training_data[\"TypeGlass\"].values[0]\n",
        "  class_id=training_data[target].values[0]\n",
        "  print('CLASS ID',class_id)\n",
        "  X=training_data.iloc[:,:-1].values.astype(\"float32\")# number of features for training data \n",
        "  n_inp=X.shape[1]# slice training data into small batches\n",
        "  train_dataset=(tf.data.Dataset.from_tensor_slices(X.reshape(X.shape[0],n_inp)).batch(BATCH_SIZE))\n",
        "  # init the generator with number of features desired for the output and noise dimension\n",
        "  generator=Generator(n_inp,NOISE_DIM)\n",
        "  critic=Critic(n_inp)\n",
        "  # Init RMSprop optimizer for the generator and the critic \n",
        "  generator_optimizer=tf.keras.optimizers.RMSprop(learning_rate)\n",
        "  critic_optimizer=tf.keras.optimizers.RMSprop(learning_rate)\n",
        "  # WD distance across epochs\n",
        "  # Gen loss across epochs\n",
        "  # Desc loss across epochs\n",
        "  epoch_wasserstein=[] \n",
        "  epoch_gen_loss=[] \n",
        "  epoch_critic_loss_real=[] \n",
        "  epoch_critic_loss_fake=[]\n",
        "  for epoch in range(epochs):\n",
        "    batch_idx=0\n",
        "    batch_wasserstein=0\n",
        "    batch_gen=0\n",
        "    batch_critic_real=0\n",
        "    batch_critic_fake=0\n",
        "    # training\n",
        "    for batch in train_dataset:\n",
        "      wasserstein,gen_loss,critic_loss_real,critic_loss_fake=train_step(batch,generator,critic,NOISE_DIM,generator_optimizer,critic_optimizer)\n",
        "      epoch_wasserstein.append(wasserstein)\n",
        "      epoch_gen_loss.append(gen_loss)\n",
        "      epoch_critic_loss_real.append(critic_loss_real)\n",
        "      epoch_critic_loss_fake.append(critic_loss_fake)\n",
        "      batch_gen+=gen_loss\n",
        "      batch_critic_real+=critic_loss_real\n",
        "      batch_critic_fake+=critic_loss_fake\n",
        "      batch_wasserstein+=wasserstein\n",
        "      batch_idx+=1\n",
        "    batch_wasserstein=batch_wasserstein/batch_idx\n",
        "    batch_gen=batch_gen/batch_idx\n",
        "    batch_critic_real=batch_critic_real/batch_idx\n",
        "    batch_critic_fake=batch_critic_fake/batch_idx\n",
        "    if epoch%50==0:\n",
        "      print(\"Epoch %d / %d completed. Gen loss: %.8f. Desc loss_real: %.8f . Desc loss_fake: %.8f\"%(epoch+1,epochs,batch_gen,batch_critic_real,batch_critic_fake))\n",
        "      \"\"\"nb_instances_to_generate = len(class_0[\"target\"]) - len(class_1[\"target\"])    \"\"\"\n",
        "  data=generate_synthetic_samples(generator,class_id,headers_name,nb_instances_to_generate,NOISE_DIM)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNFL8wXpMFeT"
      },
      "outputs": [],
      "source": [
        "# the function to generate fake data with WGAN for the given classes \n",
        "def gen_data(X_train,y_train,target,classes):\n",
        "  # count_classes=dict(y_train.value_counts())\n",
        "  count_classes=collections.Counter(y_train)\n",
        "  max_class=max(count_classes.values())\n",
        "  print('MAX CLASS',max_class)\n",
        "  new_data=pd.DataFrame()\n",
        "  tmp=X_train.copy()\n",
        "  tmp[target]=y_train\n",
        "  for c in set(classes):\n",
        "    training_data=tmp[tmp[target]==c]\n",
        "    nb_instances_to_generate=max_class-count_classes[c]\n",
        "    if nb_instances_to_generate !=0:\n",
        "      syhtnetic_data=fake_data_generation(training_data,nb_instances_to_generate,target)\n",
        "      syhtnetic_data.rename(columns={'0':target},inplace=True)\n",
        "      syhtnetic_data[target]=c\n",
        "      new_data=new_data.append(syhtnetic_data)\n",
        "  return new_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TABGAN related functions"
      ],
      "metadata": {
        "id": "lUl1nU2RBDJR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp6L7JIlpBaD"
      },
      "outputs": [],
      "source": [
        "def run_tabgan(X_train, y_train, X_test,y_test,target,classes):\n",
        "  count_classes=dict(y_train[target].value_counts())\n",
        "  max_class=max(count_classes.values())\n",
        "  new_data=pd.DataFrame()\n",
        "  new_train=pd.DataFrame()\n",
        "  new_target=pd.Series()\n",
        "  tmp=X_train.copy()\n",
        "  tmp[target]=y_train\n",
        "  tmp_test=X_test.copy()\n",
        "  tmp_test[target]=y_test\n",
        "  for c in set(classes):\n",
        "    training_data=tmp[tmp[target]==c]\n",
        "    print('CLASS',c)\n",
        "    nb_instances_to_generate=1+max_class/count_classes[c] \n",
        "    if nb_instances_to_generate !=1:\n",
        "      new_tr, new_tar = GANGenerator(gen_x_times=nb_instances_to_generate, cat_cols=None,\n",
        "            bot_filter_quantile=0.001, top_filter_quantile=0.999, is_post_process=False,\n",
        "            adversarial_model_params={\n",
        "                \"metrics\": \"AUC\", \"max_depth\": 2, \"max_bin\": 100, \n",
        "                \"learning_rate\": 0.02, \"random_state\": 42, \"n_estimators\": 500,\n",
        "            }, pregeneration_frac=2, only_generated_data=True,\n",
        "            gan_params = {\"batch_size\": 16, \"patience\": 5, \"epochs\" : 150,}).generate_data_pipe(pd.DataFrame(training_data.drop(target,1)), \n",
        "                                                                                                pd.DataFrame(training_data[target]), \n",
        "                                                                                                tmp_test[tmp_test[target]==c].drop(target,1), \n",
        "                                                                                                deep_copy=True, only_adversarial=False, \n",
        "                                                                                                use_adversarial=True)\n",
        "      new_train=new_train.append(new_tr)\n",
        "      new_target=new_target.append(new_tar)\n",
        "  new_target=pd.DataFrame(new_target,columns=[9])\n",
        "  return new_train, new_target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "dLXvSBMDBI59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VagMMEV6Lc3a"
      },
      "outputs": [],
      "source": [
        "# function to load data \n",
        "def data_loader(filename):\n",
        "  data=pd.read_csv(filename+\".csv\")\n",
        "  return data\n",
        "# check data on null values\n",
        "def check_notnull(data):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.ylabel('Number')\n",
        "    plt.title('Non-Missing Values in columns within %d instances ' % data.shape[0])\n",
        "    plt.bar(data.columns, data.notnull().sum())\n",
        "\n",
        "# functions for EDA\n",
        "def plot_displot(data):\n",
        "    fig = plt.figure(1, figsize=(20, 40))\n",
        "\n",
        "    for i in range(len(data.columns)):\n",
        "        fig.add_subplot(10, 5, i + 1)\n",
        "        sns.histplot(data.iloc[i], kde=True)\n",
        "        plt.axvline(data[data.columns[i]].mean(), c='green')\n",
        "        plt.axvline(data[data.columns[i]].median(), c='blue')\n",
        "\n",
        "def plot_scatter(data, x, y, target):\n",
        "    fig = plt.figure(1, figsize=(8, 5))\n",
        "    sns.scatterplot(data=data, x=x, y=y, hue=target)\n",
        "    plt.xlabel('ftr# {}'.format(x))\n",
        "    plt.ylabel('ftr# {}'.format(y))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_class_dist(target_column):\n",
        "    ax = target_column.value_counts().plot(kind='bar', figsize=(12, 8), \n",
        "                                           fontsize=12, \n",
        "                                           color=['#6ca5ce','#a06cce','#6cb4ce',\n",
        "                                                  '#6cce81','#c92c4c','#c726c9'])\n",
        "    ax.set_title('Target class\\n', size=20, pad=30)\n",
        "    ax.set_ylabel('Number of samples', fontsize=12)\n",
        "    for i in ax.patches:\n",
        "        ax.text(i.get_x() + 0.19, i.get_height(), str(round(i.get_height(), 2)), \n",
        "                fontsize=12)\n",
        "\n",
        "def plot_pie(data,labels,title):\n",
        "  #Usage:\n",
        "  # data = df[target].value_counts()\n",
        "  # print(df[target].value_counts(True)*100)\n",
        "  # plot_pie(data,classes,'Gallagher Dataset')\n",
        "    fig, ax = plt.subplots(figsize =(20, 10))\n",
        "    colors = sns.color_palette('pastel')\n",
        "    ax.pie(data, labels = labels, colors = colors)\n",
        "    ax.set_title(title,fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "def plot_class_dist(target_column):\n",
        "  # Usage: plot_class_dist(df[target])\n",
        "    ax = target_column.value_counts().plot(kind='bar', figsize=(12, 6),\n",
        "         fontsize=12, \n",
        "         color=['#6ca5ce','#a06cce','#6cb4ce','#6cce81','#c92c4c','#c726c9'])\n",
        "    ax.set_title('Target class\\n', size=16, pad=30)\n",
        "    ax.set_ylabel('Number of samples', fontsize=12)\n",
        "    for i in ax.patches:\n",
        "        ax.text(i.get_x() + 0.19, i.get_height(), str(round(i.get_height(), 2)),\n",
        "                fontsize=12)\n",
        "\n",
        "def fill_missing_values(data, num_features, cat_features):\n",
        "    for f in num_features:\n",
        "        median = data[f].mean()\n",
        "        data[f].fillna(median, inplace=True)\n",
        "    for col in cat_features:\n",
        "        most_frequent_category = data[col].mode()[0]\n",
        "        data[col].fillna(most_frequent_category, inplace=True)\n",
        "\n",
        "\n",
        "def encode_target(data, target):\n",
        "    label_encoder = LabelEncoder()\n",
        "    target_encoded = label_encoder.fit_transform(data[target])\n",
        "    return target_encoded\n",
        "\n",
        "\n",
        "def standardize_data(data, num_features):\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(data[num_features])\n",
        "    return scaled_data\n",
        "\n",
        "\n",
        "def transfrom_cat_features(data, cat_features):\n",
        "    for c in cat_features:\n",
        "        data = data.merge(pd.get_dummies(data[c], prefix=c), \n",
        "                          left_index=True, right_index=True)\n",
        "    data.drop(cat_features, axis=1, inplace=True)\n",
        "\n",
        "\n",
        "def split_data(data, target):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, target, \n",
        "                                                        stratify=target, \n",
        "                                                        test_size=0.33, \n",
        "                                                        random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# get the oversampler model by key from dict\n",
        "def get_oversampler(oversamplers_dict, oversampler_num, proportion):\n",
        "    if proportion == None:\n",
        "        return oversamplers_dict[oversampler_num]()\n",
        "    else:\n",
        "        return oversamplers_dict[oversampler_num](proportion=proportion)\n",
        "\n",
        "def get_filter(filters_dict, filter_num):\n",
        "    return filters_dict[filter_num]\n",
        "\n",
        "# function to preprocess input data\n",
        "def preprocess(data, target, num_features, cat_features):\n",
        "    # check_notnull(data.drop(target, 1))\n",
        "    fill_missing_values(data.drop(target, 1), num_features, cat_features)\n",
        "    data[target] = encode_target(data, target)\n",
        "    data[num_features] = standardize_data(data, num_features)\n",
        "    transfrom_cat_features(data, cat_features)\n",
        "    return data\n",
        "\n",
        "# function to print evaluation metrics values\n",
        "def print_eval_results(y_test, preds):\n",
        "    print('Classification report:')\n",
        "    print(classification_report(y_test, preds))\n",
        "    print('Geometric mean:', geometric_mean_score(y_test, preds, \n",
        "                                                  average='weighted'))\n",
        "    print('Geometric mean default:', geometric_mean_score(y_test, preds))\n",
        "    print('Cohen Kappa', cohen_kappa_score(y_test, preds))\n",
        "\n",
        "# function to get optimal DT model\n",
        "def get_model(X_train, y_train):\n",
        "    param_grid = { 'criterion':['gini','entropy'],\n",
        "                  'max_depth': np.arange(3, 200),\n",
        "                  'max_features': ['auto', 'log2'],\n",
        "                  }\n",
        "    model=DecisionTreeClassifier()\n",
        "    adb = GridSearchCV(model, param_grid, cv=5,scoring='f1_weighted')\n",
        "    adb.fit(X_train, y_train)\n",
        "    return adb.best_estimator_ \n",
        "\n",
        "# function for calculation of error per class\n",
        "def error_per_class(y_test,preds,classes):\n",
        "  cm = confusion_matrix(y_test, preds)\n",
        "  # to store the results in a dictionary for easy access later\n",
        "  per_class_accuracies = {}\n",
        "  per_class_error={}\n",
        "  # Calculate the accuracy for each one of our classes\n",
        "  for idx, cls in enumerate(classes):\n",
        "      # TN - all the samples that are not current GT class \n",
        "      # and not predicted as the current class\n",
        "      true_negatives = np.sum(np.delete(np.delete(cm, idx, axis=0), idx, axis=1))\n",
        "      # TP are all the samples of current GT class that were predicted as such\n",
        "      true_positives = cm[idx, idx]\n",
        "      # accuracy for the current class\n",
        "      per_class_accuracies[cls] = (true_positives) / np.sum(cm[idx,:])\n",
        "      per_class_error[cls] = 1-(true_positives) / np.sum(cm[idx,:])\n",
        "  print('PER CLASS ERROR', per_class_error)\n",
        "  return per_class_error\n",
        "\n",
        "# functions to perform KS test for gen/real data\n",
        "def ks_test(real, gen):\n",
        "  df_a=np.array(real.values)\n",
        "  df_b=np.array(gen.values)\n",
        "  ks_scores=ks_2samp(df_a, df_b)\n",
        "  print(\"Gen vs Real: ks statistic\",ks_scores.statistic)\n",
        "  print(\"Gen vs Real: ks pvalue\",ks_scores.pvalue)\n",
        "  print(\"Gen & Real distributions are equal\",ks_scores.pvalue>0.05)\n",
        "\n",
        "def run_kstwo(X_sample,X_train):\n",
        "    df_gen=pd.DataFrame(X_sample.copy(),columns=X_train.columns)\n",
        "    df_gen=df_gen[~df_gen.isin(X_train)].dropna()\n",
        "    df_gen['gen']='generated'\n",
        "    df_real=pd.DataFrame(X_train.copy())\n",
        "    df_real['gen']='real'\n",
        "    df=pd.concat([df_real,df_gen])\n",
        "    for col in df_gen.drop(['gen'],1).columns.to_list():\n",
        "      print('Feature',col)\n",
        "      ks_test(df_gen[col], df_real[col])\n",
        "\n",
        "# functions for filtering data points \n",
        "\n",
        "# function to find N neighbors for a point\n",
        "def get_neighbours(X_train,X_gen):\n",
        "  nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree', p=2).fit(np.array(X_train.values))\n",
        "  _,ind=nbrs.kneighbors(np.array(X_gen.values))\n",
        "  return ind\n",
        "\n",
        "# filtering\n",
        "def filter_data(X_train,y_train,X_gen,y_gen,X_test,init_error,c,y_test,classes):\n",
        "  # init empty F set\n",
        "  n=len(X_gen)\n",
        "  X_filtered=pd.DataFrame()\n",
        "  y_filtered=pd.Series()\n",
        "  i=0\n",
        "  # find n-neighbors for the data point\n",
        "  k_neighbours=get_neighbours(X_train,X_gen)\n",
        "\n",
        "  for kn in k_neighbours:\n",
        "    X_tmp=X_train.copy()\n",
        "    y_tmp=y_train.copy()\n",
        "    # find class of the neigborhood \n",
        "    max_class=max(collections.Counter(y_train[kn]))\n",
        "    # if gen_sample class equals to neighborhood's class we append sample to F set\n",
        "    if max_class==y_gen.iloc[i]:\n",
        "      X_filtered=X_filtered.append(X_gen.iloc[i])\n",
        "      y_filtered=pd.concat([pd.Series(y_filtered),pd.Series(y_gen.iloc[i])])\n",
        "    # otherwise we check whether there is an improvement in error rate\n",
        "    else:\n",
        "      X_tmp=X_tmp.append(X_gen.iloc[i])\n",
        "      y_tmp=pd.concat([pd.Series(y_tmp),pd.Series(y_gen.iloc[i])])\n",
        "      clf_model=get_model(X_tmp,y_tmp)\n",
        "      preds = clf_model.predict(X_test)\n",
        "      error=error_per_class(y_test,preds,classes)\n",
        "      # if there is an improvement\n",
        "      # we append sample to F set\n",
        "      if init_error[c]>error[c]:\n",
        "        X_filtered.append(X_gen.iloc[i])\n",
        "        y_filtered.append(y_gen.iloc[i])\n",
        "  i+=1\n",
        "  return pd.DataFrame(X_filtered),pd.Series(y_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "sWiWYpoCBWJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "   # 1. Data Upload and Pre-processing \n",
        "    \n",
        "    ### START SECTION ###\n",
        "    ### PUT HERE STRINGS FROM README SECTION TO UPLOAD THE REQUIRED DATASET ###\n",
        "    filename = 'glass.csv'\n",
        "    target = 'TypeGlass'\n",
        "    classes=[0,1,2,3,4,5]\n",
        "    data = pd.read_csv(filename, header=0)\n",
        "    cat_features = []\n",
        "    num_features = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'] \n",
        "    \n",
        "    data = preprocess(data, target, num_features, cat_features)\n",
        "    X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), \n",
        "                                                  data[target])\n",
        "    X_train.reset_index(inplace=True,drop=True)\n",
        "    X_test.reset_index(inplace=True,drop=True)\n",
        "    y_train.reset_index(inplace=True,drop=True)\n",
        "    y_test.reset_index(inplace=True,drop=True)\n",
        "    ### END SECTION ###\n",
        "\n",
        "    print('Trainset shape', X_train.shape)\n",
        "    print('Testset shape', X_test.shape)\n",
        "    print(y_train.value_counts() )\n",
        "    print(y_test.value_counts() / y_test.shape[0])\n",
        "\n",
        "    # 2. Baseline results\n",
        "\n",
        "    print('BASELINE MODEL')\n",
        "    clf_model=get_model(X_train,y_train)\n",
        "    preds = clf_model.predict(X_test)\n",
        "    print_eval_results(y_test, preds)\n",
        "    #initial error_rate per class\n",
        "    init_error=error_per_class(y_test,preds,classes)\n",
        "\n",
        "    # 3. WGAN oversampling \n",
        "    print('WGAN OVERSAMPLING')\n",
        "    X_sample=gen_data(X_train,y_train,target,classes)\n",
        "    X_sample.rename(columns={'TargetClass':target},inplace=True)\n",
        "    X_train[target]=y_train\n",
        "    X_sample=X_sample.append(X_train)\n",
        "    X_train=X_train.drop(target,1)\n",
        "    y_sample=X_sample[target]\n",
        "    X_sample=X_sample.drop(target,1)\n",
        "    clf_model=get_model(X_sample, y_sample)\n",
        "    preds = clf_model.predict(X_test)\n",
        "    print_eval_results(y_test, preds)\n",
        "    error_per_class(y_test,preds,classes)\n",
        "\n",
        "    # 4. KS test for WGAN data\n",
        "    print('KS tests for WGAN')\n",
        "    run_kstwo(X_sample,X_train)\n",
        "\n",
        "    # 5. SMOTE based oversampling \n",
        "    print('SMOTE BASED OVERSAMPLING')\n",
        "    oversamplers_dict = {1: sv.G_SMOTE, \n",
        "                         2: sv.SMOTE, \n",
        "                         3: sv.RWO_sampling, \n",
        "                         5: sv.ANS, \n",
        "                         6: sv.kmeans_SMOTE}\n",
        "    oversampler = sv.MulticlassOversampling(get_oversampler(oversamplers_dict, 2, None))\n",
        "    X_sample, y_sample = oversampler.sample(X_train.values, y_train.values)\n",
        "    clf_model=get_model(X_sample, y_sample)\n",
        "    preds = clf_model.predict(X_test)\n",
        "    print_eval_results(y_test, preds)\n",
        "    error_per_class(y_test,preds,classes)\n",
        "    # 6. CTGAN based oversampling \n",
        "    print('TABGAN OVERSAMPLING')\n",
        "    X_sample,y_sample = run_tabgan(X_train, pd.DataFrame(y_train),X_test,\n",
        "                                   pd.DataFrame(y_test),target,classes)\n",
        "    X_sample=X_sample.append(pd.DataFrame(X_train))\n",
        "    y_sample=pd.concat([pd.DataFrame(y_sample,columns=[9]),pd.DataFrame(y_train)])\n",
        "    clf_model=get_model(X_sample,y_sample)\n",
        "    preds = clf_model.predict(X_test)\n",
        "    print_eval_results(y_test, preds)\n",
        "    error_per_class(y_test,preds,classes)\n",
        "\n",
        "    # 7. Filtering\n",
        "    print('FILTERING PART')\n",
        "    X_out=X_train.copy()\n",
        "    y_out=y_train.copy()\n",
        "    for c in classes:\n",
        "      error=init_error\n",
        "      ins=y_train.index[y_train==c].tolist()\n",
        "      print('GEN DATA FOR CLASS ', c)\n",
        "      X_gan= gen_data(X_train, pd.DataFrame(y_train),target,set([c])) #1\n",
        "      y_gan = pd.Series(X_gan[target]) #2\n",
        "      X_gan=X_gan.drop(target,1) #3\n",
        "      #-----------OR-----------#\n",
        "      # to test CTGAN replace 1,2,3 with\n",
        "      # X_gan,y_gan = run_tabgan(X_train.iloc[ins], \n",
        "      #                          pd.DataFrame(y_train).iloc[ins],\n",
        "      #                          X_test,pd.DataFrame(y_test),target,set([c]))\n",
        "      #-------------------------#\n",
        "      X_filtered,y_filtered = filter_data(X_train,y_train,X_gan,pd.Series(y_gan),\n",
        "                                          X_test,init_error,c,y_test,classes)\n",
        "      X_out=X_out.append(pd.DataFrame(X_filtered))\n",
        "      y_out=pd.concat([pd.DataFrame(y_out,columns=[target]),\n",
        "                       pd.DataFrame(y_filtered,columns=[target])])\n",
        "      y_out.reset_index(inplace=True,drop=True)\n",
        "      X_out.reset_index(inplace=True,drop=True)\n",
        "      X_gan=X_filtered.append(pd.DataFrame(X_train))\n",
        "      y_gan=pd.concat([pd.DataFrame(y_filtered,columns=[target]),\n",
        "                       pd.DataFrame(y_train)])\n",
        "      clf_model=get_model(X_gan,y_gan.astype('int'))\n",
        "      preds = clf_model.predict(X_test)\n",
        "      error=error_per_class(y_test,preds,classes)\n",
        "      y_gan.reset_index(inplace=True,drop=True)\n",
        "      X_gan.reset_index(inplace=True,drop=True)\n",
        "\n",
        "    clf_model=get_model(X_out,y_out)\n",
        "    preds = clf_model.predict(X_test)\n",
        "    error=error_per_class(y_test,preds,classes)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ISkkOW-tdkMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## README "
      ],
      "metadata": {
        "id": "0FIQsP50-x5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### glass dataset <br>\n",
        "\n",
        "filename = 'glass.csv' <br>\n",
        "target = 'TypeGlass' <br>\n",
        "classes=[0,1,2,3,4,5] <br>\n",
        "data = pd.read_csv(filename, header=0) <br>\n",
        "cat_features = [] <br>\n",
        "num_features = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe'] <br>\n",
        "data = preprocess(data, target, num_features, cat_features) <br>\n",
        "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target) <br>\n",
        "\n",
        "### dermathology dataset\n",
        "filename = 'dermatology.csv' <br>\n",
        "target = 'Class' <br>\n",
        "classes=[1,2,3,4,5,6] <br>\n",
        "data = pd.read_csv(filename, header=0) <br>\n",
        "cat_features = [] <br>\n",
        "num_features = ['Erythema', 'Scaling', 'Definite_borders', 'Itching',\n",
        "'Koebner_phenomenon', 'Polygonal_papules', 'Follicular_papules',\n",
        "'Oral_mucosal', 'Knee_and_elbow', 'Scalp_involvement', 'Family_history',\n",
        "'Melanin_incontinence', 'Eosinophils', 'PNL_infiltrate', 'Fibrosis',\n",
        "'Exocytosis', 'Acanthosis', 'Hyperkeratosis', 'Parakeratosis',\n",
        "'Clubbing', 'Elongation', 'Thinning', 'Spongiform_pustule',\n",
        "'Munro_microabcess', 'Focal_hypergranulosis', 'Granular_layer',\n",
        "'Vacuolisation', 'Spongiosis', 'Saw-tooth_appearance',\n",
        "'Follicular_horn_plug', 'Perifollicular_parakeratosis',\n",
        "'Inflammatory_monoluclear', 'Band-like_infiltrate', 'Age']  <br>\n",
        "data = preprocess(data, target, num_features, cat_features) <br>\n",
        "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target) <br>\n",
        "\n",
        "### wine dataset\n",
        "filename= 'wine.csv' <br>\n",
        "target = 'Class' <br>\n",
        "classes=[1,2,3] <br>\n",
        "data = pd.read_csv(filename, header=0) <br>\n",
        "cat_features = [] <br>\n",
        "num_features= ['Alcohol', 'MalicAcid', 'Ash', 'AlcalinityOfAsh', 'Magnesium',\n",
        "'TotalPhenols', 'flavanoids', 'NonflavanoidsPhenols', 'Proanthocyanins',\n",
        "'ColorIntensity', 'Hue', 'OD280/OD315', 'Proline'] <br>\n",
        "data = preprocess(data, target, num_features, cat_features) <br>\n",
        "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target) <br>\n",
        "\n",
        "### page-blocks\n",
        "filename = 'page-blocks.csv' <br>\n",
        "target = 'Class' <br>\n",
        "classes=[0,1,2,3,4,5,6] <br>\n",
        "data = pd.read_csv(filename, header=0) <br>\n",
        "cat_features = [] <br>\n",
        "num_features = ['Height', 'Lenght', 'Area', 'Eccen', 'P_black', 'P_and', 'Mean_tr','Blackpix', 'Blackand', 'Wb_trans'] <br>\n",
        "data = preprocess(data, target, num_features, cat_features) <br>\n",
        "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target]) <br>\n",
        "\n",
        "### internet firewall\n",
        "\n",
        "num_features=[\"Bytes Sent\",\"Bytes Received\",\"Elapsed Time (sec)\", \"Bytes\",'Packets',\"pkts_sent\",\"pkts_received\"] <br>\n",
        "cat_features = ['Source Port', 'Destination Port', 'NAT Source Port',\n",
        "       'NAT Destination Port'] <br>\n",
        "\n",
        "scaler = StandardScaler() <br>\n",
        "scaler.fit(data[num_features]) <br>\n",
        "data[num_features] = scaler.transform(data[num_features]) <br>\n",
        "data=data.astype({'Source Port': 'object',\n",
        "                  'Destination Port': 'object',\n",
        "                  'NAT Source Port': 'object',\n",
        "                  'NAT Destination Port': 'object',\n",
        "                  }) <br>\n",
        "count_enc = CountFrequencyEncoder(encoding_method=\"frequency\", variables=cat_features) <br>\n",
        "count_enc.fit(data) <br>\n",
        "data = count_enc.transform(data) <br>\n",
        "data.to_csv('log2_preproc.csv', index=False) <br>\n",
        "\n",
        "filename = 'log2_preproc.csv' <br>\n",
        "target = 'Action' <br>\n",
        "classes=[0,1,2,3] <br>\n",
        "data = pd.read_csv(filename, header=0) <br>\n",
        "X_train, X_test, y_train, y_test = split_data(data.drop(target, 1), data[target]) <br>\n",
        "\n",
        "### shuttle dataset\n",
        "\n",
        "df_train=pd.read_csv('shuttle.trn', delimiter=' ',header=None) <br>\n",
        "df_test=pd.read_csv('shuttle.tst', delimiter=' ',header=None) <br>\n",
        "target=9 <br>\n",
        "num_features=[0,1,2,3,4,5,6,7,8] <br>\n",
        "cat_features=[] <br>\n",
        "df_train=preprocess(df_train, target, num_features, cat_features) <br>\n",
        "df_test=preprocess(df_test, target, num_features, cat_features) <br>\n",
        "X_train, y_train = df_train.drop(9,1),df_train[9] <br>\n",
        "X_test, y_test = df_test.drop(9,1),df_test[9] <br>\n",
        "\n",
        "### gallagher\n",
        "\n",
        "features_file = 'mobnet_features.npy' <br>\n",
        "vectors = np.load(features_file) <br>\n",
        "labels= np.load('mobnet_labels.npy') <br>\n",
        "label_enc = preprocessing.LabelEncoder() <br>\n",
        "label_enc.fit(labels) <br>\n",
        "labels = label_enc.transform(labels) <br>\n",
        "df=pd.DataFrame(vectors) <br>\n",
        "df['Class']=labels <br>\n",
        "target='Class' <br>\n",
        "classes=[11,22,0,30,31,27,26,28,3,17,29,13,6,25,10,19] <br>\n",
        "df=df[df.Class.isin(classes)] <br>\n",
        "df.reset_index(inplace=True,drop=True) <br>\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(target,1), df[target], test_size=0.33, random_state=42) <br>\n",
        "\n"
      ],
      "metadata": {
        "id": "mUuMyMpt13EZ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ImbLearn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}